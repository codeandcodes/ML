{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Notes\n",
    "Common problems as a result of tokenization:\n",
    "- LLM can't spell words\n",
    "- LLM can't do super simple string processing (like reversing a string)\n",
    "- LLM worse at non-English langauges\n",
    "- LLM bad at arithmetic\n",
    "- GPT-2 trouble coding python\n",
    "- warning about trailing whitespace\n",
    "- YAML over JSON with LLMs\n",
    "\n",
    "tiktokenizer\n",
    "\n",
    "Non english languages work less well in chatgpt.\n",
    "Why? less training data, tokenizer is also trained with more english\n",
    "Also training data - use much more tokens in foreign languages because of the way they get broken up. All the non-english text is stretched out from the perspective of the xformer. will create bigger tokens/larger groups in english\n",
    "\n",
    "Coding\n",
    "- python each space is being considered a token. if you use a lot of indentation with python, then each space gets used as context length. you're being too wasteful.\n",
    "\n",
    "cl100k_base (GPT4 tokenizer)\n",
    "- halved the tokens - why? because # of tokens in GPT4 tokenizer is roughly double (went from 50k to 100k)\n",
    "- this is a \"good thing\" maybe, because this is much denser tokens. \n",
    "- roughly able to see twice as much text as context to predict next. \n",
    "- just increasing number of tokens does not necessarily better. softmax grows. there is a sweet spot where you have just right number of tokens in your vocabulary\n",
    "- GPT4 tokenizer, 3 spaces get grouped together, 7 spaces got grouped \n",
    "- this \"densifies\" the text, which improved ability to code.\n",
    "\n",
    "Why can't we just use integer for each character?\n",
    "- the vocabulary would be very long\n",
    "- the unicode standard keeps changing as well\n",
    "- for these reasons we need something better.\n",
    "- thus we turn to encodings (UTF-8, UTF-16, UTF-32)\n",
    "- these encodings are ways we can take unicode text and convert them into byte strings\n",
    "- UTF-8 takes every single code point and converts between 1 to 4 bytes (variable length)\n",
    "- UTF-32 is fixed length not variable\n",
    "- UTF-8 manifesto; UTF-8 is the only backwards compatible encoding to ascii\n",
    "- if we use utf-8 naively, there are only 256 values, if we used this, all of our text would be stretched over very long sequences of bytes. we don't want to use raw bytes of utf-8 encoding. we want to use a variable vocabulary size which is tuned as a hyperparameter. \n",
    "- AK prefers if they could not do tokenization he would prefer it, but not proven out yet.\n",
    "\n",
    "Byte-pair encoding\n",
    "- this is a compression algorithm, can compress down until the to a vocabulary size is achieved\n",
    "- suppose the data to be reocrded is\n",
    "- aaabdaaabac - identify the pair that occurs most often, so you mint a new token, and replace every occurrence of this with the new token\n",
    "- e.g.\n",
    "- \"aa\" occurs most often, so replace with \"Z\"\n",
    "\"ZabdZabac\"\n",
    "Then we repeat with byte pair ab, replace it with \"Y\"\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "The only literal byte pair left occurs only once, and the encoding might stop here, or the process could continue with recursive byte pair encoding, replacing ZY with X\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "after we go through this process, instead of having 11 tokens with vocab length of 4, we have vocab length of 7\n",
    "we iteratively compress \n",
    "we start with byte seq of 256, we will iteratively start minting new tokens. we have an algorithm for getting up to a vocab size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Unicode! UNICODE! The very name strikes fear and awe into the hearts of programmers.\n",
      "The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely. Chapters: 00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues 00:05:50 tokenization by example in a Web UI (tiktokenizer) 00:14:56 strings in Python, Unicode code points 00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32 00:22:47 daydreaming: deleting tokenization 00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough 00:27:02 starting the implementation 00:28:35 counting consecutive pairs, finding most common pair 00:30:36 merging the most common pair 00:34:58 training the tokenizer: adding the while loop, compression ratio 00:39:20 tokenizer/LLM diagram: it is a completely separate stage 00:42:47 decoding tokens to strings 00:48:21 encoding strings to tokens 00:57:36 regex patterns to force splits across categories 01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex 01:14:59 GPT-2 encoder.py released by OpenAI walkthrough 01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences 01:25:28 minbpe exercise time! write your own GPT-4 tokenizer 01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary 01:43:27 how to set vocabulary set? revisiting gpt.py transformer 01:48:11 training new tokens, example of prompt compression 01:49:58 multimodal [image, video, audio] tokenization with vector quantization 01:51:41 revisiting and explaining the quirks of LLM tokenization 02:10:20 final recommendations 02:12:50 ??? :) Exercises: - Advised flow: reference this document and try to implement the steps before I give away the partial solutions in the video. The full solutions if you're getting stuck are in the minbpe code https://github.com/karpathy/minbpe/blob/master/exercise.md Links: - Google colab for the video: https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing - GitHub repo for the video: minBPE https://github.com/karpathy/minbpe - Playlist of the whole Zero to Hero series so far: https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ - our Discord channel: https://discord.gg/3zy8kqD9Cp - my Twitter: https://twitter.com/karpathy Supplementary links: - tiktokenizer https://tiktokenizer.vercel.app - tiktoken from OpenAI: https://github.com/openai/tiktoken - sentencepiece from Google https://github.com/google/sentencepiece\n",
      "\n",
      "length 3159\n",
      "---\n",
      "[85, 110, 105, 99, 111, 100, 101, 33, 32, 85, 78, 73, 67, 79, 68, 69, 33, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 46, 10, 84, 104, 101, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 32, 105, 115, 32, 97, 32, 110, 101, 99, 101, 115, 115, 97, 114, 121, 32, 97, 110, 100, 32, 112, 101, 114, 118, 97, 115, 105, 118, 101, 32, 99, 111, 109, 112, 111, 110, 101, 110, 116, 32, 111, 102, 32, 76, 97, 114, 103, 101, 32, 76, 97, 110, 103, 117, 97, 103, 101, 32, 77, 111, 100, 101, 108, 115, 32, 40, 76, 76, 77, 115, 41, 44, 32, 119, 104, 101, 114, 101, 32, 105, 116, 32, 116, 114, 97, 110, 115, 108, 97, 116, 101, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 115, 116, 114, 105, 110, 103, 115, 32, 97, 110, 100, 32, 116, 111, 107, 101, 110, 115, 32, 40, 116, 101, 120, 116, 32, 99, 104, 117, 110, 107, 115, 41, 46, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 115, 32, 97, 114, 101, 32, 97, 32, 99, 111, 109, 112, 108, 101, 116, 101, 108, 121, 32, 115, 101, 112, 97, 114, 97, 116, 101, 32, 115, 116, 97, 103, 101, 32, 111, 102, 32, 116, 104, 101, 32, 76, 76, 77, 32, 112, 105, 112, 101, 108, 105, 110, 101, 58, 32, 116, 104, 101, 121, 32, 104, 97, 118, 101, 32, 116, 104, 101, 105, 114, 32, 111, 119, 110, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 115, 101, 116, 115, 44, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 115, 32, 40, 66, 121, 116, 101, 32, 80, 97, 105, 114, 32, 69, 110, 99, 111, 100, 105, 110, 103, 41, 44, 32, 97, 110, 100, 32, 97, 102, 116, 101, 114, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 32, 116, 119, 111, 32, 102, 117, 110, 100, 97, 109, 101, 110, 116, 97, 108, 32, 102, 117, 110, 99, 116, 105, 111, 110, 115, 58, 32, 101, 110, 99, 111, 100, 101, 40, 41, 32, 102, 114, 111, 109, 32, 115, 116, 114, 105, 110, 103, 115, 32, 116, 111, 32, 116, 111, 107, 101, 110, 115, 44, 32, 97, 110, 100, 32, 100, 101, 99, 111, 100, 101, 40, 41, 32, 98, 97, 99, 107, 32, 102, 114, 111, 109, 32, 116, 111, 107, 101, 110, 115, 32, 116, 111, 32, 115, 116, 114, 105, 110, 103, 115, 46, 32, 73, 110, 32, 116, 104, 105, 115, 32, 108, 101, 99, 116, 117, 114, 101, 32, 119, 101, 32, 98, 117, 105, 108, 100, 32, 102, 114, 111, 109, 32, 115, 99, 114, 97, 116, 99, 104, 32, 116, 104, 101, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 32, 117, 115, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 71, 80, 84, 32, 115, 101, 114, 105, 101, 115, 32, 102, 114, 111, 109, 32, 79, 112, 101, 110, 65, 73, 46, 32, 73, 110, 32, 116, 104, 101, 32, 112, 114, 111, 99, 101, 115, 115, 44, 32, 119, 101, 32, 119, 105, 108, 108, 32, 115, 101, 101, 32, 116, 104, 97, 116, 32, 97, 32, 108, 111, 116, 32, 111, 102, 32, 119, 101, 105, 114, 100, 32, 98, 101, 104, 97, 118, 105, 111, 114, 115, 32, 97, 110, 100, 32, 112, 114, 111, 98, 108, 101, 109, 115, 32, 111, 102, 32, 76, 76, 77, 115, 32, 97, 99, 116, 117, 97, 108, 108, 121, 32, 116, 114, 97, 99, 101, 32, 98, 97, 99, 107, 32, 116, 111, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 46, 32, 87, 101, 39, 108, 108, 32, 103, 111, 32, 116, 104, 114, 111, 117, 103, 104, 32, 97, 32, 110, 117, 109, 98, 101, 114, 32, 111, 102, 32, 116, 104, 101, 115, 101, 32, 105, 115, 115, 117, 101, 115, 44, 32, 100, 105, 115, 99, 117, 115, 115, 32, 119, 104, 121, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 105, 115, 32, 97, 116, 32, 102, 97, 117, 108, 116, 44, 32, 97, 110, 100, 32, 119, 104, 121, 32, 115, 111, 109, 101, 111, 110, 101, 32, 111, 117, 116, 32, 116, 104, 101, 114, 101, 32, 105, 100, 101, 97, 108, 108, 121, 32, 102, 105, 110, 100, 115, 32, 97, 32, 119, 97, 121, 32, 116, 111, 32, 100, 101, 108, 101, 116, 101, 32, 116, 104, 105, 115, 32, 115, 116, 97, 103, 101, 32, 101, 110, 116, 105, 114, 101, 108, 121, 46, 32, 67, 104, 97, 112, 116, 101, 114, 115, 58, 32, 48, 48, 58, 48, 48, 58, 48, 48, 32, 105, 110, 116, 114, 111, 58, 32, 84, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 44, 32, 71, 80, 84, 45, 50, 32, 112, 97, 112, 101, 114, 44, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 45, 114, 101, 108, 97, 116, 101, 100, 32, 105, 115, 115, 117, 101, 115, 32, 48, 48, 58, 48, 53, 58, 53, 48, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 98, 121, 32, 101, 120, 97, 109, 112, 108, 101, 32, 105, 110, 32, 97, 32, 87, 101, 98, 32, 85, 73, 32, 40, 116, 105, 107, 116, 111, 107, 101, 110, 105, 122, 101, 114, 41, 32, 48, 48, 58, 49, 52, 58, 53, 54, 32, 115, 116, 114, 105, 110, 103, 115, 32, 105, 110, 32, 80, 121, 116, 104, 111, 110, 44, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 111, 100, 101, 32, 112, 111, 105, 110, 116, 115, 32, 48, 48, 58, 49, 56, 58, 49, 53, 32, 85, 110, 105, 99, 111, 100, 101, 32, 98, 121, 116, 101, 32, 101, 110, 99, 111, 100, 105, 110, 103, 115, 44, 32, 65, 83, 67, 73, 73, 44, 32, 85, 84, 70, 45, 56, 44, 32, 85, 84, 70, 45, 49, 54, 44, 32, 85, 84, 70, 45, 51, 50, 32, 48, 48, 58, 50, 50, 58, 52, 55, 32, 100, 97, 121, 100, 114, 101, 97, 109, 105, 110, 103, 58, 32, 100, 101, 108, 101, 116, 105, 110, 103, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 48, 48, 58, 50, 51, 58, 53, 48, 32, 66, 121, 116, 101, 32, 80, 97, 105, 114, 32, 69, 110, 99, 111, 100, 105, 110, 103, 32, 40, 66, 80, 69, 41, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 119, 97, 108, 107, 116, 104, 114, 111, 117, 103, 104, 32, 48, 48, 58, 50, 55, 58, 48, 50, 32, 115, 116, 97, 114, 116, 105, 110, 103, 32, 116, 104, 101, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 97, 116, 105, 111, 110, 32, 48, 48, 58, 50, 56, 58, 51, 53, 32, 99, 111, 117, 110, 116, 105, 110, 103, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 101, 32, 112, 97, 105, 114, 115, 44, 32, 102, 105, 110, 100, 105, 110, 103, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 112, 97, 105, 114, 32, 48, 48, 58, 51, 48, 58, 51, 54, 32, 109, 101, 114, 103, 105, 110, 103, 32, 116, 104, 101, 32, 109, 111, 115, 116, 32, 99, 111, 109, 109, 111, 110, 32, 112, 97, 105, 114, 32, 48, 48, 58, 51, 52, 58, 53, 56, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 116, 104, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 58, 32, 97, 100, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 105, 108, 101, 32, 108, 111, 111, 112, 44, 32, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 114, 97, 116, 105, 111, 32, 48, 48, 58, 51, 57, 58, 50, 48, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 47, 76, 76, 77, 32, 100, 105, 97, 103, 114, 97, 109, 58, 32, 105, 116, 32, 105, 115, 32, 97, 32, 99, 111, 109, 112, 108, 101, 116, 101, 108, 121, 32, 115, 101, 112, 97, 114, 97, 116, 101, 32, 115, 116, 97, 103, 101, 32, 48, 48, 58, 52, 50, 58, 52, 55, 32, 100, 101, 99, 111, 100, 105, 110, 103, 32, 116, 111, 107, 101, 110, 115, 32, 116, 111, 32, 115, 116, 114, 105, 110, 103, 115, 32, 48, 48, 58, 52, 56, 58, 50, 49, 32, 101, 110, 99, 111, 100, 105, 110, 103, 32, 115, 116, 114, 105, 110, 103, 115, 32, 116, 111, 32, 116, 111, 107, 101, 110, 115, 32, 48, 48, 58, 53, 55, 58, 51, 54, 32, 114, 101, 103, 101, 120, 32, 112, 97, 116, 116, 101, 114, 110, 115, 32, 116, 111, 32, 102, 111, 114, 99, 101, 32, 115, 112, 108, 105, 116, 115, 32, 97, 99, 114, 111, 115, 115, 32, 99, 97, 116, 101, 103, 111, 114, 105, 101, 115, 32, 48, 49, 58, 49, 49, 58, 51, 56, 32, 116, 105, 107, 116, 111, 107, 101, 110, 32, 108, 105, 98, 114, 97, 114, 121, 32, 105, 110, 116, 114, 111, 44, 32, 100, 105, 102, 102, 101, 114, 101, 110, 99, 101, 115, 32, 98, 101, 116, 119, 101, 101, 110, 32, 71, 80, 84, 45, 50, 47, 71, 80, 84, 45, 52, 32, 114, 101, 103, 101, 120, 32, 48, 49, 58, 49, 52, 58, 53, 57, 32, 71, 80, 84, 45, 50, 32, 101, 110, 99, 111, 100, 101, 114, 46, 112, 121, 32, 114, 101, 108, 101, 97, 115, 101, 100, 32, 98, 121, 32, 79, 112, 101, 110, 65, 73, 32, 119, 97, 108, 107, 116, 104, 114, 111, 117, 103, 104, 32, 48, 49, 58, 49, 56, 58, 50, 54, 32, 115, 112, 101, 99, 105, 97, 108, 32, 116, 111, 107, 101, 110, 115, 44, 32, 116, 105, 107, 116, 111, 107, 101, 110, 32, 104, 97, 110, 100, 108, 105, 110, 103, 32, 111, 102, 44, 32, 71, 80, 84, 45, 50, 47, 71, 80, 84, 45, 52, 32, 100, 105, 102, 102, 101, 114, 101, 110, 99, 101, 115, 32, 48, 49, 58, 50, 53, 58, 50, 56, 32, 109, 105, 110, 98, 112, 101, 32, 101, 120, 101, 114, 99, 105, 115, 101, 32, 116, 105, 109, 101, 33, 32, 119, 114, 105, 116, 101, 32, 121, 111, 117, 114, 32, 111, 119, 110, 32, 71, 80, 84, 45, 52, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 48, 49, 58, 50, 56, 58, 52, 50, 32, 115, 101, 110, 116, 101, 110, 99, 101, 112, 105, 101, 99, 101, 32, 108, 105, 98, 114, 97, 114, 121, 32, 105, 110, 116, 114, 111, 44, 32, 117, 115, 101, 100, 32, 116, 111, 32, 116, 114, 97, 105, 110, 32, 76, 108, 97, 109, 97, 32, 50, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 48, 49, 58, 52, 51, 58, 50, 55, 32, 104, 111, 119, 32, 116, 111, 32, 115, 101, 116, 32, 118, 111, 99, 97, 98, 117, 108, 97, 114, 121, 32, 115, 101, 116, 63, 32, 114, 101, 118, 105, 115, 105, 116, 105, 110, 103, 32, 103, 112, 116, 46, 112, 121, 32, 116, 114, 97, 110, 115, 102, 111, 114, 109, 101, 114, 32, 48, 49, 58, 52, 56, 58, 49, 49, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 110, 101, 119, 32, 116, 111, 107, 101, 110, 115, 44, 32, 101, 120, 97, 109, 112, 108, 101, 32, 111, 102, 32, 112, 114, 111, 109, 112, 116, 32, 99, 111, 109, 112, 114, 101, 115, 115, 105, 111, 110, 32, 48, 49, 58, 52, 57, 58, 53, 56, 32, 109, 117, 108, 116, 105, 109, 111, 100, 97, 108, 32, 91, 105, 109, 97, 103, 101, 44, 32, 118, 105, 100, 101, 111, 44, 32, 97, 117, 100, 105, 111, 93, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 119, 105, 116, 104, 32, 118, 101, 99, 116, 111, 114, 32, 113, 117, 97, 110, 116, 105, 122, 97, 116, 105, 111, 110, 32, 48, 49, 58, 53, 49, 58, 52, 49, 32, 114, 101, 118, 105, 115, 105, 116, 105, 110, 103, 32, 97, 110, 100, 32, 101, 120, 112, 108, 97, 105, 110, 105, 110, 103, 32, 116, 104, 101, 32, 113, 117, 105, 114, 107, 115, 32, 111, 102, 32, 76, 76, 77, 32, 116, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 48, 50, 58, 49, 48, 58, 50, 48, 32, 102, 105, 110, 97, 108, 32, 114, 101, 99, 111, 109, 109, 101, 110, 100, 97, 116, 105, 111, 110, 115, 32, 48, 50, 58, 49, 50, 58, 53, 48, 32, 63, 63, 63, 32, 58, 41, 32, 69, 120, 101, 114, 99, 105, 115, 101, 115, 58, 32, 45, 32, 65, 100, 118, 105, 115, 101, 100, 32, 102, 108, 111, 119, 58, 32, 114, 101, 102, 101, 114, 101, 110, 99, 101, 32, 116, 104, 105, 115, 32, 100, 111, 99, 117, 109, 101, 110, 116, 32, 97, 110, 100, 32, 116, 114, 121, 32, 116, 111, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 32, 116, 104, 101, 32, 115, 116, 101, 112, 115, 32, 98, 101, 102, 111, 114, 101, 32, 73, 32, 103, 105, 118, 101, 32, 97, 119, 97, 121, 32, 116, 104, 101, 32, 112, 97, 114, 116, 105, 97, 108, 32, 115, 111, 108, 117, 116, 105, 111, 110, 115, 32, 105, 110, 32, 116, 104, 101, 32, 118, 105, 100, 101, 111, 46, 32, 84, 104, 101, 32, 102, 117, 108, 108, 32, 115, 111, 108, 117, 116, 105, 111, 110, 115, 32, 105, 102, 32, 121, 111, 117, 39, 114, 101, 32, 103, 101, 116, 116, 105, 110, 103, 32, 115, 116, 117, 99, 107, 32, 97, 114, 101, 32, 105, 110, 32, 116, 104, 101, 32, 109, 105, 110, 98, 112, 101, 32, 99, 111, 100, 101, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 99, 111, 109, 47, 107, 97, 114, 112, 97, 116, 104, 121, 47, 109, 105, 110, 98, 112, 101, 47, 98, 108, 111, 98, 47, 109, 97, 115, 116, 101, 114, 47, 101, 120, 101, 114, 99, 105, 115, 101, 46, 109, 100, 32, 76, 105, 110, 107, 115, 58, 32, 45, 32, 71, 111, 111, 103, 108, 101, 32, 99, 111, 108, 97, 98, 32, 102, 111, 114, 32, 116, 104, 101, 32, 118, 105, 100, 101, 111, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 99, 111, 108, 97, 98, 46, 114, 101, 115, 101, 97, 114, 99, 104, 46, 103, 111, 111, 103, 108, 101, 46, 99, 111, 109, 47, 100, 114, 105, 118, 101, 47, 49, 121, 48, 75, 110, 67, 70, 90, 118, 71, 86, 102, 95, 111, 100, 83, 102, 99, 78, 65, 119, 115, 54, 107, 99, 68, 68, 55, 72, 115, 73, 48, 76, 63, 117, 115, 112, 61, 115, 104, 97, 114, 105, 110, 103, 32, 45, 32, 71, 105, 116, 72, 117, 98, 32, 114, 101, 112, 111, 32, 102, 111, 114, 32, 116, 104, 101, 32, 118, 105, 100, 101, 111, 58, 32, 109, 105, 110, 66, 80, 69, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 99, 111, 109, 47, 107, 97, 114, 112, 97, 116, 104, 121, 47, 109, 105, 110, 98, 112, 101, 32, 45, 32, 80, 108, 97, 121, 108, 105, 115, 116, 32, 111, 102, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 90, 101, 114, 111, 32, 116, 111, 32, 72, 101, 114, 111, 32, 115, 101, 114, 105, 101, 115, 32, 115, 111, 32, 102, 97, 114, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 119, 119, 119, 46, 121, 111, 117, 116, 117, 98, 101, 46, 99, 111, 109, 47, 119, 97, 116, 99, 104, 63, 118, 61, 86, 77, 106, 45, 51, 83, 49, 116, 107, 117, 48, 38, 108, 105, 115, 116, 61, 80, 76, 65, 113, 104, 73, 114, 106, 107, 120, 98, 117, 87, 73, 50, 51, 118, 57, 99, 84, 104, 115, 65, 57, 71, 118, 67, 65, 85, 104, 82, 118, 75, 90, 32, 45, 32, 111, 117, 114, 32, 68, 105, 115, 99, 111, 114, 100, 32, 99, 104, 97, 110, 110, 101, 108, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 100, 105, 115, 99, 111, 114, 100, 46, 103, 103, 47, 51, 122, 121, 56, 107, 113, 68, 57, 67, 112, 32, 45, 32, 109, 121, 32, 84, 119, 105, 116, 116, 101, 114, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 116, 119, 105, 116, 116, 101, 114, 46, 99, 111, 109, 47, 107, 97, 114, 112, 97, 116, 104, 121, 32, 83, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 108, 105, 110, 107, 115, 58, 32, 45, 32, 116, 105, 107, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 104, 116, 116, 112, 115, 58, 47, 47, 116, 105, 107, 116, 111, 107, 101, 110, 105, 122, 101, 114, 46, 118, 101, 114, 99, 101, 108, 46, 97, 112, 112, 32, 45, 32, 116, 105, 107, 116, 111, 107, 101, 110, 32, 102, 114, 111, 109, 32, 79, 112, 101, 110, 65, 73, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 99, 111, 109, 47, 111, 112, 101, 110, 97, 105, 47, 116, 105, 107, 116, 111, 107, 101, 110, 32, 45, 32, 115, 101, 110, 116, 101, 110, 99, 101, 112, 105, 101, 99, 101, 32, 102, 114, 111, 109, 32, 71, 111, 111, 103, 108, 101, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 99, 111, 109, 47, 103, 111, 111, 103, 108, 101, 47, 115, 101, 110, 116, 101, 110, 99, 101, 112, 105, 101, 99, 101, 10]\n",
      "length: 3159\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "# unicode codepoints is a definnition of ~150K characters, what they look like\n",
    "# and what integers represent these characters\n",
    "\n",
    "ord(\"h\") # 104\n",
    "ord(\"y\") # 121\n",
    "\n",
    "[ord(x) for x in \"hello world\"]\n",
    "\n",
    "list(\"hello world\".encode(\"utf-16\"))\n",
    "# notice the 0, 101, 0 108 0 111 etc\n",
    "# it's wasteful \n",
    "# same thing is observable with utf-32\n",
    "\n",
    "text = \"Unicode! UNICODE! The very name strikes fear and awe into the hearts of programmers.\"\n",
    "text += \"\"\"\n",
    "The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely. Chapters: 00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues 00:05:50 tokenization by example in a Web UI (tiktokenizer) 00:14:56 strings in Python, Unicode code points 00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32 00:22:47 daydreaming: deleting tokenization 00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough 00:27:02 starting the implementation 00:28:35 counting consecutive pairs, finding most common pair 00:30:36 merging the most common pair 00:34:58 training the tokenizer: adding the while loop, compression ratio 00:39:20 tokenizer/LLM diagram: it is a completely separate stage 00:42:47 decoding tokens to strings 00:48:21 encoding strings to tokens 00:57:36 regex patterns to force splits across categories 01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex 01:14:59 GPT-2 encoder.py released by OpenAI walkthrough 01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences 01:25:28 minbpe exercise time! write your own GPT-4 tokenizer 01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary 01:43:27 how to set vocabulary set? revisiting gpt.py transformer 01:48:11 training new tokens, example of prompt compression 01:49:58 multimodal [image, video, audio] tokenization with vector quantization 01:51:41 revisiting and explaining the quirks of LLM tokenization 02:10:20 final recommendations 02:12:50 ??? :) Exercises: - Advised flow: reference this document and try to implement the steps before I give away the partial solutions in the video. The full solutions if you're getting stuck are in the minbpe code https://github.com/karpathy/minbpe/blob/master/exercise.md Links: - Google colab for the video: https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing - GitHub repo for the video: minBPE https://github.com/karpathy/minbpe - Playlist of the whole Zero to Hero series so far: https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ - our Discord channel: https://discord.gg/3zy8kqD9Cp - my Twitter: https://twitter.com/karpathy Supplementary links: - tiktokenizer https://tiktokenizer.vercel.app - tiktoken from OpenAI: https://github.com/openai/tiktoken - sentencepiece from Google https://github.com/google/sentencepiece\n",
    "\"\"\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens)) # convert all bytes to list of integers in range 0..255 for convenience\n",
    "tokens\n",
    "\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))\n",
    "\n",
    "def get_sorted_pairs(token_list):\n",
    "  # for ints in token_list, consider i and i+1 pair\n",
    "  # as you iterate across list, for each i and i+1, add count to map\n",
    "  # keep track of the max (or should there be a priority queue)\n",
    "  i = 0\n",
    "  pairs = {}\n",
    "  while i < len(token_list) - 1:\n",
    "    pair = (token_list[i], token_list[i+1])\n",
    "    if pair in pairs:\n",
    "      pairs[pair] += 1\n",
    "    else:\n",
    "      pairs[pair] = 1\n",
    "    i += 1\n",
    "  sorted_pairs = sorted(((v,k) for k,v in pairs.items()), reverse=True)\n",
    "  return sorted_pairs # 4 occurrences of 101, 32\n",
    "\n",
    "sorted_pairs = get_sorted_pairs(tokens)\n",
    "\n",
    "chr(101), chr(32) # most common pair\n",
    "\n",
    "# mint new token with 101, 32\n",
    "# create a new token with 256, since values go from 0-255\n",
    "# every time we see 101, 32 replace with 256\n",
    "\n",
    "def replace_most_common(token_list, target, val):\n",
    "  i = 0\n",
    "  while i < len(token_list) - 1:\n",
    "    if (token_list[i], token_list[i+1]) == target:\n",
    "      token_list[i] = val\n",
    "      token_list = token_list[:i+1] + token_list[i+2:]\n",
    "    i += 1\n",
    "  return token_list\n",
    "\n",
    "# how to do the list manipulation\n",
    "# list_a = [1,2,3,4,5]\n",
    "# list_a\n",
    "# i = 3\n",
    "# list_a[i] = 6 \n",
    "# list_a = list_a[:i+1] + list_a[i+2:] # works at pos 0, pos 1 to len-1, at pos = len-1, no match can be found so ok\n",
    "# list_a\n",
    "\n",
    "# sorted_pairs[0][1]\n",
    "orig_tokens = list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing  (32, 116)  with val  256\n",
      "replacing  (101, 32)  with val  257\n",
      "replacing  (105, 110)  with val  258\n",
      "replacing  (101, 110)  with val  259\n",
      "replacing  (115, 32)  with val  260\n",
      "replacing  (101, 114)  with val  261\n",
      "replacing  (99, 111)  with val  262\n",
      "replacing  (258, 103)  with val  263\n",
      "replacing  (256, 111)  with val  264\n",
      "replacing  (107, 259)  with val  265\n",
      "replacing  (256, 104)  with val  266\n",
      "replacing  (97, 116)  with val  267\n",
      "replacing  (32, 115)  with val  268\n",
      "replacing  (111, 110)  with val  269\n",
      "replacing  (97, 114)  with val  270\n",
      "replacing  (44, 32)  with val  271\n",
      "replacing  (105, 122)  with val  272\n",
      "replacing  (265, 272)  with val  273\n",
      "replacing  (114, 111)  with val  274\n",
      "[85, 110, 105, 262, 100, 101, 33, 32, 85, 78, 73, 67, 79, 68, 69, 33, 32, 84, 104, 257, 118, 261, 121, 32, 110, 97, 109, 257, 115, 116, 114, 105, 107, 101, 260, 102, 101, 270, 32, 97, 110, 100, 32, 97, 119, 257, 258, 116, 111, 266, 257, 104, 101, 270, 116, 260, 111, 102, 32, 112, 274, 103, 114, 97, 109, 109, 261, 115, 46, 10, 84, 104, 257, 84, 111, 273, 261, 32, 105, 260, 97, 32, 110, 101, 99, 101, 115, 115, 270, 121, 32, 97, 110, 100, 32, 112, 261, 118, 97, 115, 105, 118, 257, 262, 109, 112, 269, 259, 116, 32, 111, 102, 32, 76, 270, 103, 257, 76, 97, 110, 103, 117, 97, 103, 257, 77, 111, 100, 101, 108, 260, 40, 76, 76, 77, 115, 41, 271, 119, 104, 261, 257, 105, 116, 256, 114, 97, 110, 115, 108, 267, 101, 260, 98, 101, 116, 119, 101, 259, 268, 116, 114, 263, 260, 97, 110, 100, 264, 265, 260, 40, 116, 101, 120, 116, 32, 99, 104, 117, 110, 107, 115, 41, 46, 32, 84, 111, 273, 261, 260, 270, 257, 97, 32, 262, 109, 112, 108, 101, 116, 101, 108, 121, 268, 101, 112, 270, 267, 257, 115, 116, 97, 103, 257, 111, 102, 266, 257, 76, 76, 77, 32, 112, 105, 112, 101, 108, 258, 101, 58, 266, 101, 121, 32, 104, 97, 118, 101, 266, 101, 105, 114, 32, 111, 119, 110, 256, 114, 97, 258, 263, 268, 101, 116, 115, 44, 256, 114, 97, 258, 263, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 260, 40, 66, 121, 116, 257, 80, 97, 105, 114, 32, 69, 110, 262, 100, 263, 41, 271, 97, 110, 100, 32, 97, 102, 116, 261, 256, 114, 97, 258, 263, 32, 105, 109, 112, 108, 101, 109, 259, 116, 256, 119, 111, 32, 102, 117, 110, 100, 97, 109, 259, 116, 97, 108, 32, 102, 117, 110, 99, 116, 105, 269, 115, 58, 32, 259, 262, 100, 101, 40, 41, 32, 102, 274, 109, 268, 116, 114, 263, 115, 264, 264, 265, 115, 271, 97, 110, 100, 32, 100, 101, 262, 100, 101, 40, 41, 32, 98, 97, 99, 107, 32, 102, 274, 109, 264, 265, 115, 264, 268, 116, 114, 263, 115, 46, 32, 73, 110, 266, 105, 260, 108, 101, 99, 116, 117, 114, 257, 119, 257, 98, 117, 105, 108, 100, 32, 102, 274, 109, 268, 99, 114, 267, 99, 104, 266, 257, 84, 111, 273, 261, 32, 117, 115, 101, 100, 32, 258, 266, 257, 71, 80, 84, 268, 261, 105, 101, 260, 102, 274, 109, 32, 79, 112, 259, 65, 73, 46, 32, 73, 110, 266, 257, 112, 274, 99, 101, 115, 115, 271, 119, 257, 119, 105, 108, 108, 268, 101, 101, 266, 267, 32, 97, 32, 108, 111, 116, 32, 111, 102, 32, 119, 101, 105, 114, 100, 32, 98, 101, 104, 97, 118, 105, 111, 114, 260, 97, 110, 100, 32, 112, 274, 98, 108, 101, 109, 260, 111, 102, 32, 76, 76, 77, 260, 97, 99, 116, 117, 97, 108, 108, 121, 256, 114, 97, 99, 257, 98, 97, 99, 107, 264, 264, 273, 267, 105, 269, 46, 32, 87, 101, 39, 108, 108, 32, 103, 111, 266, 274, 117, 103, 104, 32, 97, 32, 110, 117, 109, 98, 261, 32, 111, 102, 266, 101, 115, 257, 105, 115, 115, 117, 101, 115, 271, 100, 105, 115, 99, 117, 115, 260, 119, 104, 121, 264, 273, 267, 105, 269, 32, 105, 260, 267, 32, 102, 97, 117, 108, 116, 271, 97, 110, 100, 32, 119, 104, 121, 268, 111, 109, 101, 269, 257, 111, 117, 116, 266, 261, 257, 105, 100, 101, 97, 108, 108, 121, 32, 102, 258, 100, 260, 97, 32, 119, 97, 121, 264, 32, 100, 101, 108, 101, 116, 101, 266, 105, 260, 115, 116, 97, 103, 257, 259, 116, 105, 114, 101, 108, 121, 46, 32, 67, 104, 97, 112, 116, 261, 115, 58, 32, 48, 48, 58, 48, 48, 58, 48, 48, 32, 258, 116, 274, 58, 32, 84, 111, 273, 267, 105, 269, 271, 71, 80, 84, 45, 50, 32, 112, 97, 112, 261, 44, 264, 273, 267, 105, 269, 45, 114, 101, 108, 267, 101, 100, 32, 105, 115, 115, 117, 101, 260, 48, 48, 58, 48, 53, 58, 53, 48, 264, 273, 267, 105, 269, 32, 98, 121, 32, 101, 120, 97, 109, 112, 108, 257, 258, 32, 97, 32, 87, 101, 98, 32, 85, 73, 32, 40, 116, 105, 107, 116, 111, 273, 261, 41, 32, 48, 48, 58, 49, 52, 58, 53, 54, 268, 116, 114, 263, 260, 258, 32, 80, 121, 116, 104, 269, 271, 85, 110, 105, 262, 100, 257, 262, 100, 257, 112, 111, 258, 116, 260, 48, 48, 58, 49, 56, 58, 49, 53, 32, 85, 110, 105, 262, 100, 257, 98, 121, 116, 257, 259, 262, 100, 263, 115, 271, 65, 83, 67, 73, 73, 271, 85, 84, 70, 45, 56, 271, 85, 84, 70, 45, 49, 54, 271, 85, 84, 70, 45, 51, 50, 32, 48, 48, 58, 50, 50, 58, 52, 55, 32, 100, 97, 121, 100, 114, 101, 97, 109, 263, 58, 32, 100, 101, 108, 101, 116, 263, 264, 273, 267, 105, 269, 32, 48, 48, 58, 50, 51, 58, 53, 48, 32, 66, 121, 116, 257, 80, 97, 105, 114, 32, 69, 110, 262, 100, 263, 32, 40, 66, 80, 69, 41, 32, 97, 108, 103, 111, 114, 105, 116, 104, 109, 32, 119, 97, 108, 107, 116, 104, 274, 117, 103, 104, 32, 48, 48, 58, 50, 55, 58, 48, 50, 268, 116, 270, 116, 263, 266, 257, 105, 109, 112, 108, 101, 109, 259, 116, 267, 105, 269, 32, 48, 48, 58, 50, 56, 58, 51, 53, 32, 262, 117, 110, 116, 263, 32, 262, 110, 115, 101, 99, 117, 116, 105, 118, 257, 112, 97, 105, 114, 115, 271, 102, 258, 100, 263, 32, 109, 111, 115, 116, 32, 262, 109, 109, 269, 32, 112, 97, 105, 114, 32, 48, 48, 58, 51, 48, 58, 51, 54, 32, 109, 261, 103, 263, 266, 257, 109, 111, 115, 116, 32, 262, 109, 109, 269, 32, 112, 97, 105, 114, 32, 48, 48, 58, 51, 52, 58, 53, 56, 256, 114, 97, 258, 263, 266, 101, 264, 273, 261, 58, 32, 97, 100, 100, 263, 266, 257, 119, 104, 105, 108, 257, 108, 111, 111, 112, 271, 262, 109, 112, 114, 101, 115, 115, 105, 269, 32, 114, 267, 105, 111, 32, 48, 48, 58, 51, 57, 58, 50, 48, 264, 273, 261, 47, 76, 76, 77, 32, 100, 105, 97, 103, 114, 97, 109, 58, 32, 105, 116, 32, 105, 260, 97, 32, 262, 109, 112, 108, 101, 116, 101, 108, 121, 268, 101, 112, 270, 267, 257, 115, 116, 97, 103, 257, 48, 48, 58, 52, 50, 58, 52, 55, 32, 100, 101, 262, 100, 263, 264, 265, 115, 264, 268, 116, 114, 263, 260, 48, 48, 58, 52, 56, 58, 50, 49, 32, 259, 262, 100, 263, 268, 116, 114, 263, 115, 264, 264, 265, 260, 48, 48, 58, 53, 55, 58, 51, 54, 32, 114, 101, 103, 101, 120, 32, 112, 267, 116, 261, 110, 115, 264, 32, 102, 111, 114, 99, 257, 115, 112, 108, 105, 116, 260, 97, 99, 274, 115, 260, 99, 267, 101, 103, 111, 114, 105, 101, 260, 48, 49, 58, 49, 49, 58, 51, 56, 256, 105, 107, 116, 111, 265, 32, 108, 105, 98, 114, 270, 121, 32, 258, 116, 274, 271, 100, 105, 102, 102, 261, 259, 99, 101, 260, 98, 101, 116, 119, 101, 259, 32, 71, 80, 84, 45, 50, 47, 71, 80, 84, 45, 52, 32, 114, 101, 103, 101, 120, 32, 48, 49, 58, 49, 52, 58, 53, 57, 32, 71, 80, 84, 45, 50, 32, 259, 262, 100, 261, 46, 112, 121, 32, 114, 101, 108, 101, 97, 115, 101, 100, 32, 98, 121, 32, 79, 112, 259, 65, 73, 32, 119, 97, 108, 107, 116, 104, 274, 117, 103, 104, 32, 48, 49, 58, 49, 56, 58, 50, 54, 268, 112, 101, 99, 105, 97, 108, 264, 265, 115, 44, 256, 105, 107, 116, 111, 265, 32, 104, 97, 110, 100, 108, 263, 32, 111, 102, 271, 71, 80, 84, 45, 50, 47, 71, 80, 84, 45, 52, 32, 100, 105, 102, 102, 261, 259, 99, 101, 260, 48, 49, 58, 50, 53, 58, 50, 56, 32, 109, 258, 98, 112, 257, 101, 120, 261, 99, 105, 115, 101, 256, 105, 109, 101, 33, 32, 119, 114, 105, 116, 257, 121, 111, 117, 114, 32, 111, 119, 110, 32, 71, 80, 84, 45, 52, 264, 273, 261, 32, 48, 49, 58, 50, 56, 58, 52, 50, 268, 259, 116, 259, 99, 101, 112, 105, 101, 99, 257, 108, 105, 98, 114, 270, 121, 32, 258, 116, 274, 271, 117, 115, 101, 100, 264, 256, 114, 97, 258, 32, 76, 108, 97, 109, 97, 32, 50, 32, 118, 111, 99, 97, 98, 117, 108, 270, 121, 32, 48, 49, 58, 52, 51, 58, 50, 55, 32, 104, 111, 119, 264, 268, 101, 116, 32, 118, 111, 99, 97, 98, 117, 108, 270, 121, 268, 101, 116, 63, 32, 114, 101, 118, 105, 115, 105, 116, 263, 32, 103, 112, 116, 46, 112, 121, 256, 114, 97, 110, 115, 102, 111, 114, 109, 261, 32, 48, 49, 58, 52, 56, 58, 49, 49, 256, 114, 97, 258, 263, 32, 110, 101, 119, 264, 265, 115, 271, 101, 120, 97, 109, 112, 108, 257, 111, 102, 32, 112, 274, 109, 112, 116, 32, 262, 109, 112, 114, 101, 115, 115, 105, 269, 32, 48, 49, 58, 52, 57, 58, 53, 56, 32, 109, 117, 108, 116, 105, 109, 111, 100, 97, 108, 32, 91, 105, 109, 97, 103, 101, 271, 118, 105, 100, 101, 111, 271, 97, 117, 100, 105, 111, 93, 264, 273, 267, 105, 269, 32, 119, 105, 116, 104, 32, 118, 101, 99, 116, 111, 114, 32, 113, 117, 97, 110, 116, 272, 267, 105, 269, 32, 48, 49, 58, 53, 49, 58, 52, 49, 32, 114, 101, 118, 105, 115, 105, 116, 263, 32, 97, 110, 100, 32, 101, 120, 112, 108, 97, 258, 263, 266, 257, 113, 117, 105, 114, 107, 260, 111, 102, 32, 76, 76, 77, 264, 273, 267, 105, 269, 32, 48, 50, 58, 49, 48, 58, 50, 48, 32, 102, 258, 97, 108, 32, 114, 101, 262, 109, 109, 259, 100, 267, 105, 269, 260, 48, 50, 58, 49, 50, 58, 53, 48, 32, 63, 63, 63, 32, 58, 41, 32, 69, 120, 261, 99, 105, 115, 101, 115, 58, 32, 45, 32, 65, 100, 118, 105, 115, 101, 100, 32, 102, 108, 111, 119, 58, 32, 114, 101, 102, 261, 259, 99, 101, 266, 105, 260, 100, 111, 99, 117, 109, 259, 116, 32, 97, 110, 100, 256, 114, 121, 264, 32, 105, 109, 112, 108, 101, 109, 259, 116, 266, 257, 115, 116, 101, 112, 260, 98, 101, 102, 111, 114, 257, 73, 32, 103, 105, 118, 257, 97, 119, 97, 121, 266, 257, 112, 270, 116, 105, 97, 108, 268, 111, 108, 117, 116, 105, 269, 260, 258, 266, 257, 118, 105, 100, 101, 111, 46, 32, 84, 104, 257, 102, 117, 108, 108, 268, 111, 108, 117, 116, 105, 269, 260, 105, 102, 32, 121, 111, 117, 39, 114, 257, 103, 101, 116, 116, 263, 268, 116, 117, 99, 107, 32, 270, 257, 258, 266, 257, 109, 258, 98, 112, 257, 262, 100, 257, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 262, 109, 47, 107, 270, 112, 267, 104, 121, 47, 109, 258, 98, 112, 101, 47, 98, 108, 111, 98, 47, 109, 97, 115, 116, 261, 47, 101, 120, 261, 99, 105, 115, 101, 46, 109, 100, 32, 76, 258, 107, 115, 58, 32, 45, 32, 71, 111, 111, 103, 108, 257, 262, 108, 97, 98, 32, 102, 111, 114, 266, 257, 118, 105, 100, 101, 111, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 262, 108, 97, 98, 46, 114, 101, 115, 101, 270, 99, 104, 46, 103, 111, 111, 103, 108, 101, 46, 262, 109, 47, 100, 114, 105, 118, 101, 47, 49, 121, 48, 75, 110, 67, 70, 90, 118, 71, 86, 102, 95, 111, 100, 83, 102, 99, 78, 65, 119, 115, 54, 107, 99, 68, 68, 55, 72, 115, 73, 48, 76, 63, 117, 115, 112, 61, 115, 104, 270, 263, 32, 45, 32, 71, 105, 116, 72, 117, 98, 32, 114, 101, 112, 111, 32, 102, 111, 114, 266, 257, 118, 105, 100, 101, 111, 58, 32, 109, 258, 66, 80, 69, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 262, 109, 47, 107, 270, 112, 267, 104, 121, 47, 109, 258, 98, 112, 257, 45, 32, 80, 108, 97, 121, 108, 105, 115, 116, 32, 111, 102, 266, 257, 119, 104, 111, 108, 257, 90, 261, 111, 264, 32, 72, 261, 111, 268, 261, 105, 101, 260, 115, 111, 32, 102, 270, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 119, 119, 119, 46, 121, 111, 117, 116, 117, 98, 101, 46, 262, 109, 47, 119, 267, 99, 104, 63, 118, 61, 86, 77, 106, 45, 51, 83, 49, 116, 107, 117, 48, 38, 108, 105, 115, 116, 61, 80, 76, 65, 113, 104, 73, 114, 106, 107, 120, 98, 117, 87, 73, 50, 51, 118, 57, 99, 84, 104, 115, 65, 57, 71, 118, 67, 65, 85, 104, 82, 118, 75, 90, 32, 45, 32, 111, 117, 114, 32, 68, 105, 115, 262, 114, 100, 32, 99, 104, 97, 110, 110, 101, 108, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 100, 105, 115, 262, 114, 100, 46, 103, 103, 47, 51, 122, 121, 56, 107, 113, 68, 57, 67, 112, 32, 45, 32, 109, 121, 32, 84, 119, 105, 116, 116, 261, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 116, 119, 105, 116, 116, 261, 46, 262, 109, 47, 107, 270, 112, 267, 104, 121, 32, 83, 117, 112, 112, 108, 101, 109, 259, 116, 270, 121, 32, 108, 258, 107, 115, 58, 32, 45, 256, 105, 107, 116, 111, 273, 261, 32, 104, 116, 116, 112, 115, 58, 47, 47, 116, 105, 107, 116, 111, 273, 261, 46, 118, 261, 99, 101, 108, 46, 97, 112, 112, 32, 45, 256, 105, 107, 116, 111, 265, 32, 102, 274, 109, 32, 79, 112, 259, 65, 73, 58, 32, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 262, 109, 47, 111, 112, 259, 97, 105, 47, 116, 105, 107, 116, 111, 265, 32, 45, 268, 259, 116, 259, 99, 101, 112, 105, 101, 99, 257, 102, 274, 109, 32, 71, 111, 111, 103, 108, 257, 104, 116, 116, 112, 115, 58, 47, 47, 103, 105, 116, 104, 117, 98, 46, 262, 109, 47, 103, 111, 111, 103, 108, 101, 47, 115, 259, 116, 259, 99, 101, 112, 105, 101, 99, 101, 10]\n",
      "2534\n",
      "{(32, 116): 256, (101, 32): 257, (105, 110): 258, (101, 110): 259, (115, 32): 260, (101, 114): 261, (99, 111): 262, (258, 103): 263, (256, 111): 264, (107, 259): 265, (256, 104): 266, (97, 116): 267, (32, 115): 268, (111, 110): 269, (97, 114): 270, (44, 32): 271, (105, 122): 272, (265, 272): 273, (114, 111): 274}\n"
     ]
    }
   ],
   "source": [
    "vocab_count_desired = 275\n",
    "new_token_count = 256\n",
    "merges = {}\n",
    "while new_token_count < vocab_count_desired:\n",
    "  sorted_pairs = get_sorted_pairs(tokens)\n",
    "  target = sorted_pairs[0][1] # always take the max\n",
    "  merges[target] = new_token_count\n",
    "  print(\"replacing \", target, \" with val \", new_token_count)\n",
    "  tokens = replace_most_common(tokens, target, new_token_count)\n",
    "  new_token_count += 1\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length  3159\n",
      "merged length  2534\n",
      "compression ratio: 1.247X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length \", len(orig_tokens))\n",
    "print(\"merged length \", len(tokens))\n",
    "print(f\"compression ratio: {len(orig_tokens) / len(tokens):.3f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is a completely separate part to the LLM\n",
    "tokenizer has its own training set. You will train tokenizer, performing byte pair encoding to train the vocabulary of this tokenizer\n",
    "\n",
    "Has a preprocessing stage, trained using BPE.\n",
    "Once you have vocab, we can do encoding and decoding.\n",
    "tokenizer is a translation layer between raw text and turn it into a token sequence, and can take a token sequence and convert it back into raw text.\n",
    "\n",
    "how to do encoding and decoding. you choose different data sets which will influence your compression ratio. \n",
    "LLM\n",
    "token sequence\n",
    "tokenizer\n",
    "raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b' t', 257: b'e ', 258: b'in', 259: b'en', 260: b's ', 261: b'er', 262: b'co', 263: b'ing', 264: b' to', 265: b'ken', 266: b' th', 267: b'at', 268: b' s', 269: b'on', 270: b'ar', 271: b', ', 272: b'iz', 273: b'keniz', 274: b'ro'}\n",
      "The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# merges: {(32, 116): 256, (101, 32): 257\n",
    "# vocab: 255: b'\\xff', 256: b' t', 257: b'e ' ... 273: b'keniz', 274: b'ro'\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "  vocab[idx] = vocab[p0] + vocab[p1]\n",
    "print(vocab)\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "def encode(text):\n",
    "  # given raw text, encode using merges dictionary\n",
    "  # should go backwards from the end to capture the longest token possible\n",
    "  i = 0\n",
    "  encoded = []\n",
    "  # first need to convert all text into bytes\n",
    "  bytes = text.encode(\"utf-8\")\n",
    "  while i < len(bytes):\n",
    "    j = len(vocab) - 1\n",
    "    while i < len(bytes) and j >= 0: # find longest match\n",
    "      p = 0\n",
    "      while p < len(vocab[j]) and (i+p) < len(bytes) and bytes[i+p] == vocab[j][p]:\n",
    "        p += 1\n",
    "        # text = \"ro\", vocab[274] = \"ro\", \n",
    "        # p=0\n",
    "        # 'r' matches, p =1\n",
    "        # p=1\n",
    "        # 'o' matches, p =2\n",
    "        # p=2 OOB\n",
    "      if p == len(vocab[j]): # full match, replace string char with vocab index and jump forward p spaces\n",
    "        # print(\"found match \", vocab[j])\n",
    "        i += p\n",
    "        encoded.append(j)\n",
    "      else: # no match, decrement j\n",
    "        j -= 1\n",
    "  return encoded\n",
    "  \n",
    "\n",
    "text = \"The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), \"\n",
    "text2 = decode(encode(text))\n",
    "print(text2)\n",
    "print(text2 == text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE in GPT tried to enforce non-merging across certain categories. Because \"dog.\" \"dog!\" \"dog,\"\n",
    "would maybe get merged as per BPE algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', '     ', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "#\\p{L} is any character from any language\n",
    "#\\p{N} is any number\n",
    "#[^\\s\\p{L}\\p{N}]+ match punctuation\n",
    "#\\s+(?!\\S) captures multiple spaces (using negative lookahead assertion)\n",
    "#\\s+ spaces\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 how's are      you!!!?\"))\n",
    "\n",
    "# this is saying we're never going to merge 'e ' \n",
    "# this chunks up the text to enforce that some merges\n",
    "# don't actually happen. trying to not merge across\n",
    "# letters, punctuation, etc\n",
    "\n",
    "# why do to apostrophes? these are common contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "# import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPT4 they changed the regex\n",
    "special tokens and pattern change\n",
    "\n",
    "load encoder.json\n",
    "and open vocab.bpe\n",
    "\n",
    "What's in these files?\n",
    "in encoder.json is exactly equivalent to our vocab object\n",
    "vocab[idx] = vocab[p0] + vocab[p1]\n",
    "vocab.bpe is actually our merges\n",
    "\n",
    "# saving and loading the two variables that were critical to us (encoder and decoder)\n",
    "\n",
    "OPenAI also has a byte_encoder and a byte_decoder\n",
    "they have a separate layer used serially with tokenizer byte_encode then encode and byte_decode then decode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling special tokens\n",
    "# 256 raw byte tokens\n",
    "# 50000 merges\n",
    "but size is 50257\n",
    "so what is that special otken?\n",
    "called '<|endoftext|>'\n",
    "\n",
    "used to delimit documents in the training set.\n",
    "tokens in documents range from 0 ... 50256\n",
    "we insert a special token in between tokens\n",
    "\n",
    "we use this for the LLM that hte document has ended and what follows is unrelated to the document.\n",
    "\n",
    "The LLM needs to learn this that what follows is uninformative to what came before.\n",
    "\n",
    "# GPT4\n",
    "has fill in the middle special tokens\n",
    "special_tokens = {\n",
    "    ENDOFTEXT: 100257,\n",
    "    FIM_PREFIX: 100258,\n",
    "    FIM_MIDDLE: 100259, \n",
    "    FIM_SUFFIX: 100260, \n",
    "    ENDOFPROMPT: 100276,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentencepiece\n",
    "\n",
    "commonly used because it can efficienctly train and inference BPE tokenizers.\n",
    "used by llama and mistral\n",
    "\n",
    "under google/sentencepiece\n",
    "\n",
    "One big difference between is that they think differently about the order\n",
    "\n",
    "with tiktoken, we take code points, encode into UTF bytes, then merge bytes\n",
    "\n",
    "with sentencepiece, takes code points, then starts merging those code points. BPE is running on level of code points vs BPE bytes\n",
    "\n",
    "Code points will get mapped to unknown token (\"unk\") or will do a byte_fallback\n",
    "\n",
    "sentencepiece been there for a while, a huge config and accumulated baggage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
